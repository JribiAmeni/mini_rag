import os
import re
import numpy as np
from groq import Groq
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import PyPDF2

# ============================================
# CONFIGURATION
# ============================================
GROQ_API_KEY = "" #your api key
CHUNK_SIZE = 500  # Characters per chunk
CHUNK_OVERLAP = 50  # Overlap between chunks
TOP_K_RESULTS = 3  # Number of relevant passages to retrieve

# ============================================
# 1. TEXT EXTRACTION
# ============================================

def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF file
    """
    try:
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
        return text
    except Exception as e:
        print(f"Error reading PDF {pdf_path}: {str(e)}")
        return ""

def extract_text_from_txt(txt_path):
    """
    Extract text from a TXT file
    """
    try:
        with open(txt_path, 'r', encoding='utf-8') as file:
            return file.read()
    except Exception as e:
        print(f"Error reading TXT {txt_path}: {str(e)}")
        return ""

def load_documents(folder_path):
    """
    Load all PDF and TXT documents from a folder
    Returns: list of (filename, text) tuples (used to store multiple items in a single variable)
    """
    documents = []
    
    if not os.path.exists(folder_path):
        print(f"‚ö†Ô∏è  Folder '{folder_path}' does not exist!")
        return documents
    
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        
        if filename.endswith('.pdf'):
            print(f"üìÑ Loading PDF: {filename}")
            text = extract_text_from_pdf(file_path)
            if text:
                documents.append((filename, text))
                
        elif filename.endswith('.txt'):
            print(f"üìÑ Loading TXT: {filename}")
            text = extract_text_from_txt(file_path)
            if text:
                documents.append((filename, text))
    
    return documents

# ============================================
# 2. TEXT SEGMENTATION
# ============================================

def clean_text(text):
    """
    Clean text by removing extra whitespace and special characters
    """
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text)
    # Remove special characters but keep basic punctuation
    text = re.sub(r'[^\w\s.,;:!?()-]', '', text)
    return text.strip()

def segment_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    """
    Segment text into overlapping chunks
    """
    text = clean_text(text)
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        
        # Try to break at sentence boundary
        if end < len(text):
            last_period = chunk.rfind('.')
            last_question = chunk.rfind('?')
            last_exclamation = chunk.rfind('!')
            boundary = max(last_period, last_question, last_exclamation)
            
            if boundary > chunk_size * 0.5:  # Only break if we're past halfway
                chunk = chunk[:boundary + 1]
                end = start + boundary + 1
        
        chunks.append(chunk.strip())
        start = end - overlap
    
    return chunks


# ============================================
# 3. EMBEDDING GENERATION
# ============================================

class EmbeddingModel:
    """
    Lightweight embedding model using SentenceTransformers
    """
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        """
        Initialize the embedding model
        all-MiniLM-L6-v2: Fast, lightweight, good quality (80MB)
        """
        print(f"\nüì¶ Loading embedding model: {model_name}")
        self.model = SentenceTransformer(model_name)
        print("‚úì Model loaded successfully")
    
    def encode(self, texts):
        """
        Generate embeddings for a list of texts
        """
        return self.model.encode(texts, show_progress_bar=True)

# ============================================
# 4. VECTOR INDEX
# ============================================

class VectorIndex:
    """
    Simple vector index for storing and searching embeddings
    """
    def __init__(self):
        self.chunks = []
        self.embeddings = None
    
    def build(self, chunks, embedding_model):
        """
        Build the vector index from chunks
        """
        print("\nüî® Building vector index...")
        self.chunks = chunks
        
        # Extract texts for embedding
        texts = [chunk['text'] for chunk in chunks]
        
        # Generate embeddings
        self.embeddings = embedding_model.encode(texts)
        
        print(f"‚úì Index built with {len(self.chunks)} chunks")
    
    def search(self, query, embedding_model, top_k=TOP_K_RESULTS):
        """
        Search for most relevant chunks given a query
        """
        # Generate query embedding
        query_embedding = embedding_model.encode([query])[0]
        
        # Calculate cosine similarity
        similarities = cosine_similarity(
            [query_embedding],
            self.embeddings
        )[0]
        
        # Get top-k results
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                'chunk': self.chunks[idx],
                'similarity': float(similarities[idx])
            })
        
        return results

# ============================================
# 5. RAG SYSTEM
# ============================================

class RAGSystem:
    """
    Complete RAG system combining retrieval and generation
    """
    def __init__(self, groq_api_key):
        self.groq_client = Groq(api_key=groq_api_key)
        self.embedding_model = EmbeddingModel()
        self.index = VectorIndex()
    
    def load_and_index_documents(self, folder_path):
        """
        Load documents, segment, and build index
        """
        print("\n" + "=" * 60)
        print("LOADING AND INDEXING DOCUMENTS")
        print("=" * 60)
        
        # Load documents
        documents = load_documents(folder_path)
        if not documents:
            print("‚ö†Ô∏è  No documents found!")
            return False
        
        # Segment documents
        chunks = segment_all_documents(documents)
        
        # Build index
        self.index.build(chunks, self.embedding_model)
        
        print("\n‚úÖ System ready to answer questions!")
        return True
    
    def retrieve(self, query, top_k=TOP_K_RESULTS):
        """
        Retrieve relevant passages for a query
        """
        return self.index.search(query, self.embedding_model, top_k)
    
    def generate_response(self, query, relevant_chunks):
        """
        Generate a contextualized response using retrieved chunks
        """
        # Build context from retrieved chunks
        context = "\n\n".join([
            f"[Source: {chunk['chunk']['filename']}]\n{chunk['chunk']['text']}"
            for chunk in relevant_chunks
        ])
        
        # Create prompt
        prompt = f"""Based on the following context, answer the question. If the answer is not in the context, say so.

Context:
{context}

Question: {query}

Answer:"""
        
        # Generate response with Groq
        try:
            response = self.groq_client.chat.completions.create(
                messages=[
                    {
                        "role": "system",
                        "content": "You are a helpful assistant that answers questions based on provided context. Be precise and cite sources when possible."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                model="llama-3.3-70b-versatile",
                temperature=0.7,  
                max_tokens=1024,
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error generating response: {str(e)}"
    
    def query(self, question):
        """
        Complete RAG pipeline: retrieve + generate
        """
        print("\n" + "=" * 60)
        print(f"Question: {question}")
        print("=" * 60)
        
        # Retrieve relevant chunks
        print("\nüîç Retrieving relevant passages...")
        relevant_chunks = self.retrieve(question)
        
        # Display retrieved chunks
        print("\nüìö Retrieved passages:")
        for i, chunk in enumerate(relevant_chunks, 1):
            print(f"\n{i}. [{chunk['chunk']['filename']}] (similarity: {chunk['similarity']:.3f})")
            print(f"   {chunk['chunk']['text'][:200]}...")
        
        # Generate response
        print("\nüí≠ Generating response...")
        response = self.generate_response(question, relevant_chunks)
        
        print("\n" + "=" * 60)
        print("RESPONSE:")
        print("=" * 60)
        print(response)
        print("\n")
        
        return response

